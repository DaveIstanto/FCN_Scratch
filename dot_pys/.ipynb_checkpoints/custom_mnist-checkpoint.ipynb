{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification using custom FCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mnist data\n",
    "from mnist import MNIST\n",
    "\n",
    "mndata = MNIST('../dataset')\n",
    "images, labels = mndata.load_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to np array\n",
    "import numpy as np\n",
    "\n",
    "images_array = np.array(images)\n",
    "labels_array = np.array(labels).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to onehot-encoded vectors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "ohe.fit(labels_array)\n",
    "labels_array = ohe.transform(labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 100 images as sample and tests\n",
    "sample_images = images_array[:1000, :]\n",
    "sample_labels = labels_array[:1000, :]\n",
    "keras_sample_labels = np.argmax(sample_labels, axis = 1)\n",
    "\n",
    "test_images = images_array[1000:1100, :]\n",
    "test_labels = labels_array[1000:1100, :]\n",
    "keras_test_labels = np.argmax(test_labels, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized X\n",
    "normalized_sample_images = sample_images / np.amax(sample_images)\n",
    "normalized_test_images = test_images / np.amax(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Keras as Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'dense_7',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'units': 300,\n",
       " 'activation': 'sigmoid',\n",
       " 'use_bias': True,\n",
       " 'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "  'config': {'seed': None}},\n",
       " 'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       " 'kernel_regularizer': None,\n",
       " 'bias_regularizer': None,\n",
       " 'activity_regularizer': None,\n",
       " 'kernel_constraint': None,\n",
       " 'bias_constraint': None}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = keras_model.layers[0].get_config()\n",
    "config['activation'] = 'sigmoid'\n",
    "\n",
    "keras_model.layers[0].from_config(config).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 329us/sample - loss: 22.4862 - accuracy: 0.1330 - val_loss: 20.1823 - val_accuracy: 0.1400\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 71us/sample - loss: 21.0917 - accuracy: 0.5510 - val_loss: 19.6509 - val_accuracy: 0.7600\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 20.7806 - accuracy: 0.8910 - val_loss: 19.5301 - val_accuracy: 0.9300\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 65us/sample - loss: 20.6973 - accuracy: 0.9680 - val_loss: 19.4906 - val_accuracy: 0.9400\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 63us/sample - loss: 20.6796 - accuracy: 0.9690 - val_loss: 19.4848 - val_accuracy: 0.9400\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 64us/sample - loss: 20.6747 - accuracy: 0.9680 - val_loss: 19.4813 - val_accuracy: 0.9400\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 67us/sample - loss: 20.6717 - accuracy: 0.9690 - val_loss: 19.4789 - val_accuracy: 0.9400\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 68us/sample - loss: 20.6695 - accuracy: 0.9720 - val_loss: 19.4779 - val_accuracy: 0.9400\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 65us/sample - loss: 20.6686 - accuracy: 0.9790 - val_loss: 19.4774 - val_accuracy: 0.9400\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 20.6680 - accuracy: 0.9780 - val_loss: 19.4766 - val_accuracy: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10a77cfd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Setup layers\n",
    "\n",
    "keras_model = keras.Sequential([\n",
    "    keras.layers.Dense(300, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(300, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "keras_model.compile(optimizer='SGD', loss = tf.keras.losses.MSE, metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "keras_model.fit(sample_images, keras_sample_labels, validation_data = (test_images, keras_test_labels), epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Custom Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Layers as lys\n",
    "\n",
    "class Custom_FCNN:\n",
    "    def __init__(self, lr):\n",
    "        self.lin_1 = lys.Linear(784, 300, lr)\n",
    "        self.relu_1 = lys.Relu()\n",
    "        self.lin_2 = lys.Linear(300, 10, lr)\n",
    "        self.sigmoid_1 = lys.Sigmoid()\n",
    "        self.MSE_1 = lys.MSE()\n",
    "        self.layers = dict()\n",
    "\n",
    "    \n",
    "    def forward(self, x_input, y):\n",
    "        self.layers['x'] = x_input\n",
    "        h1 = self.lin_1.forward(x_input)\n",
    "        self.layers['h1'] = h1\n",
    "        act_h1 = self.relu_1.forward(h1)\n",
    "        self.layers['act_h1'] = (act_h1)\n",
    "        h2 = self.lin_2.forward(act_h1)\n",
    "        self.layers['h2'] = h2 \n",
    "        yhat = self.sigmoid_1.forward(h2)\n",
    "        self.layers['yhat'] = yhat\n",
    "        loss = self.MSE_1.forward(yhat, y)\n",
    "        self.layers['loss'] = loss\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y):\n",
    "        g = self.MSE_1.backward(self.layers['yhat'], y)\n",
    "        g = self.sigmoid_1.backward(g, self.layers['yhat'])\n",
    "        g = self.lin_2.backward(g, self.layers['act_h1'])\n",
    "        g = self.relu_1.backward(g, self.layers['act_h1'])\n",
    "        g = self.lin_1.backward(g, self.layers['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCNN_1 = Custom_FCNN(lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2820088087962375\n",
      "[0.58448311 0.62508014 0.41618597 0.55529488 0.47187744 0.5076461\n",
      " 0.55915404 0.56467713 0.41076131 0.51896328]\n",
      "0.2820088087962375\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Test forward\n",
    "\n",
    "\n",
    "sample_index = 2\n",
    "\n",
    "print(FCNN_1.forward(normalized_sample_images[sample_index], sample_labels[sample_index]))\n",
    "\n",
    "# Test backward\n",
    "FCNN_1.backward(sample_labels[sample_index])\n",
    "\n",
    "yhat = FCNN_1.layers['yhat']\n",
    "loss = FCNN_1.layers['loss']\n",
    "predicted = np.argmax(yhat)\n",
    "actual = np.argmax(sample_labels[sample_index])\n",
    "print(yhat)\n",
    "print(loss)\n",
    "\n",
    "print(predicted)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.07596609981863343\n",
      "Accuracy: 0.5\n",
      "Average Loss: 0.041024822738675464\n",
      "Accuracy: 0.774\n",
      "Average Loss: 0.029742224120408563\n",
      "Accuracy: 0.843\n",
      "Average Loss: 0.024818745839024554\n",
      "Accuracy: 0.864\n",
      "Average Loss: 0.021570208339962652\n",
      "Accuracy: 0.881\n",
      "Average Loss: 0.019076071911183108\n",
      "Accuracy: 0.898\n",
      "Average Loss: 0.017104617414736144\n",
      "Accuracy: 0.911\n",
      "Average Loss: 0.015423674360999225\n",
      "Accuracy: 0.918\n",
      "Average Loss: 0.014120173607286648\n",
      "Accuracy: 0.924\n",
      "Average Loss: 0.012992772704845354\n",
      "Accuracy: 0.929\n"
     ]
    }
   ],
   "source": [
    "# Train using custom FCNN, SGD, 1 sample minibatch\n",
    "\n",
    "def train(FCNN, train_X, train_labels, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_right_count = 0\n",
    "        for i in range(len(train_X)):\n",
    "            # Forward \n",
    "            loss = FCNN.forward(train_X[i], train_labels[i])\n",
    "            \n",
    "            # Backward, backpropagation\n",
    "            FCNN.backward(train_labels[i])\n",
    "            \n",
    "            # Calculate losses\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Calculate acc\n",
    "            actual_label = np.argmax(train_labels[i])\n",
    "            predicted_label = np.argmax(FCNN.layers['yhat'])\n",
    "        \n",
    "            if actual_label == predicted_label:\n",
    "                epoch_right_count += 1\n",
    "            \n",
    "        print(f'Average Loss: {epoch_loss / len(train_X)}')\n",
    "        print(f'Accuracy: {epoch_right_count / len(train_X)}')\n",
    "\n",
    "FCNN_NEW = Custom_FCNN(lr = 0.1)\n",
    "train(FCNN_NEW, normalized_sample_images, sample_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# Test using test sample\n",
    "def test(FCNN, test_X, test_labels):\n",
    "    total_loss = 0\n",
    "    right_count = 0\n",
    "    for i in range(len(test_X)):\n",
    "        # Forward, check loss\n",
    "        loss = FCNN.forward(test_X[i], test_labels[i])\n",
    "        \n",
    "        # Calculate losses\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Calculate acc\n",
    "        actual_label = np.argmax(test_labels[i])\n",
    "        predicted_label = np.argmax(FCNN.layers['yhat'])\n",
    "        \n",
    "        if predicted_label == actual_label:\n",
    "            right_count += 1\n",
    "        \n",
    "    print(right_count)\n",
    "    \n",
    "test(FCNN_NEW, normalized_test_images, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
